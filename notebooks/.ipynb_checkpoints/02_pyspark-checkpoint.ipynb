{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bd6558d",
   "metadata": {},
   "source": [
    "# ETL con PySpark - Market Data\n",
    "\n",
    "### Importar librerías y crear SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dcbf10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SparkSession creada: ETL Market Data\n",
      "Versión de Spark: 4.0.1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import DoubleType, DateType\n",
    "import os\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:\\\\hadoop\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL Market Data\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"✅ SparkSession creada: {spark.sparkContext.appName}\")\n",
    "print(f\"Versión de Spark: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cc190e",
   "metadata": {},
   "source": [
    "## EXTRACT: Cargar dataset original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b15d2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset cargado\n",
      "Filas: 10000\n",
      "Columnas: 15\n",
      "\n",
      "Esquema:\n",
      "root\n",
      " |-- market_type: string (nullable = true)\n",
      " |-- symbol: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- open: string (nullable = true)\n",
      " |-- close: string (nullable = true)\n",
      " |-- high: string (nullable = true)\n",
      " |-- low: string (nullable = true)\n",
      " |-- volume: string (nullable = true)\n",
      " |-- market_cap: string (nullable = true)\n",
      " |-- sector: string (nullable = true)\n",
      " |-- exchange: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- last_updated: string (nullable = true)\n",
      "\n",
      "+-----------+------+---------+----------+------------------+------------------+------------------+-----------------+------------------+------------------+----------+--------+-------+--------+-------------------+\n",
      "|market_type|symbol|name     |date      |open              |close             |high              |low              |volume            |market_cap        |sector    |exchange|country|currency|last_updated       |\n",
      "+-----------+------+---------+----------+------------------+------------------+------------------+-----------------+------------------+------------------+----------+--------+-------+--------+-------------------+\n",
      "|stock      |MSFT  |MSFT Inc |08-03-2024|47536.20817743171 |49741.830640201704|51230.74908681415 |45530.2244971417 |1560789.2088416903|1129874955353.6138|Finance   |NASDAQ  |US     |usd     |2024-08-03 23:00:00|\n",
      "|stock      |GOOGL |GOOGL Inc|2023-06-13|43310.14552728901 |44186.00670200739 |45750.35168540758 |41189.2141231024 |9699128.611767782 |183670000382.91638|Retail    |NASDAQ  |Global |USD     |2023-06-13 12:00:00|\n",
      "|crypto     |BTC   |BTC Coin |13/08/2024|10624.832142807025|9948.72087971569  |10722.264249378351|9602.625893481061|5248039.559890746 |333814843116.16656|blockchain|COINBASE|US     |$       |2024-08-13 02:00:00|\n",
      "|crypto     |SOL   |SOL Coin |08-15-2024|14568.544718500116|14894.45149823145 |14998.335725320594|14052.9236013979 |3664252.0710936235|175618282363.6357 |Crypto    |BINANCE |US     |USDT    |2024-08-15 22:00:00|\n",
      "|crypto     |SOL   |SOL Coin |29/03/2024|39260.94631003675 |36902.728007201345|40270.412843903185|36150.67729195225|465457.67678725725|185147643765.7292 |blockchain|BINANCE |Global |USD     |2024-03-29 19:00:00|\n",
      "+-----------+------+---------+----------+------------------+------------------+------------------+-----------------+------------------+------------------+----------+--------+-------+--------+-------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df_raw = spark.read.csv(\"../data/market_data.csv\", header=True, inferSchema=False)\n",
    "\n",
    "# Cachear para reutilizar\n",
    "df_raw.cache()\n",
    "\n",
    "print(f\"✅ Dataset cargado\")\n",
    "print(f\"Filas: {df_raw.count()}\")\n",
    "print(f\"Columnas: {len(df_raw.columns)}\")\n",
    "print(f\"\\nEsquema:\")\n",
    "df_raw.printSchema()\n",
    "df_raw.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11a33e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras filas:\n",
      "+-----------+------+---------+----------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------------+--------+-------+--------+-------------------+\n",
      "|market_type|symbol|name     |date      |open              |close             |high              |low               |volume            |market_cap        |sector                 |exchange|country|currency|last_updated       |\n",
      "+-----------+------+---------+----------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------------+--------+-------+--------+-------------------+\n",
      "|stock      |MSFT  |MSFT Inc |08-03-2024|47536.20817743171 |49741.830640201704|51230.74908681415 |45530.2244971417  |1560789.2088416903|1129874955353.6138|Finance                |NASDAQ  |US     |usd     |2024-08-03 23:00:00|\n",
      "|stock      |GOOGL |GOOGL Inc|2023-06-13|43310.14552728901 |44186.00670200739 |45750.35168540758 |41189.2141231024  |9699128.611767782 |183670000382.91638|Retail                 |NASDAQ  |Global |USD     |2023-06-13 12:00:00|\n",
      "|crypto     |BTC   |BTC Coin |13/08/2024|10624.832142807025|9948.72087971569  |10722.264249378351|9602.625893481061 |5248039.559890746 |333814843116.16656|blockchain             |COINBASE|US     |$       |2024-08-13 02:00:00|\n",
      "|crypto     |SOL   |SOL Coin |08-15-2024|14568.544718500116|14894.45149823145 |14998.335725320594|14052.9236013979  |3664252.0710936235|175618282363.6357 |Crypto                 |BINANCE |US     |USDT    |2024-08-15 22:00:00|\n",
      "|crypto     |SOL   |SOL Coin |29/03/2024|39260.94631003675 |36902.728007201345|40270.412843903185|36150.67729195225 |465457.67678725725|185147643765.7292 |blockchain             |BINANCE |Global |USD     |2024-03-29 19:00:00|\n",
      "|stock      |GOOGL |GOOGL Inc|2024-03-13|8534.500943127703 |7792.087425151899 |8939.414168758143 |7778.697515006511 |8084165.083816495 |1826559107185.3008|Technology             |NASDAQ  |Global |USDT    |2024-03-13 00:00:00|\n",
      "|stock      |TSLA  |TSLA Inc |2023-10-27|4892.63 USD       |5072.905748066181 |5184.548353842046 |4677.851920438645 |4952273.92420259  |920795169828.3783 |TECHNOLOGY             |NASDAQ  |Global |$       |2023-10-27 05:00:00|\n",
      "|stock      |AMZN  |AMZN Inc |08-24-2023|45466.92689991832 |43273.42031124052 |46973.069513532784|41784.189516242805|5201160.143756931 |112821678134.503  |Artificial Intelligence|NYSE    |US     |$       |2023-08-24 16:00:00|\n",
      "|stock      |AMZN  |AMZN Inc |07-22-2023|9250.874231721096 |10119.687898240996|10511.893010845839|9222.889847597311 |8948378.676926062 |1644663735236.4062|tech                   |Unknown |Global |usd     |2023-07-22 22:00:00|\n",
      "|stock      |MSFT  |MSFT Inc |2023-04-25|46094.49300880561 |42300.847111357834|46546.17954288686 |40281.46238746324 |3253977.9773018803|1797720936645.1445|finance                |NYSE    |Global |USDT    |2023-04-25 08:00:00|\n",
      "+-----------+------+---------+----------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------------+--------+-------+--------+-------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Mostrar las primeras 10 filas del DataFrame\n",
    "print(\"Primeras filas:\")\n",
    "df_raw.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6131a37",
   "metadata": {},
   "source": [
    "## EXPLORACIÓN: Análisis del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7a1954",
   "metadata": {},
   "source": [
    "### VALORES NULOS Y PROBLEMÁTICOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea9da87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VALORES NULOS Y PROBLEMÁTICOS ===\n",
      "\n",
      "symbol: 213 valores problemáticos\n",
      "exchange: 499 valores problemáticos\n"
     ]
    }
   ],
   "source": [
    "print(\"=== VALORES NULOS Y PROBLEMÁTICOS ===\\n\")\n",
    "\n",
    "valores_problematicos = ['?', 'N/A', 'n/a', 'NULL', 'null', 'Unknown', 'unknown', '', ' ']\n",
    "\n",
    "for col_name in df_raw.columns:\n",
    "    null_count = df_raw.filter(\n",
    "        col(col_name).isNull() | col(col_name).isin(valores_problematicos)\n",
    "    ).count()\n",
    "    if null_count > 0:\n",
    "        print(f\"{col_name}: {null_count} valores problemáticos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8cee87",
   "metadata": {},
   "source": [
    "### VALORES ÚNICOS EN COLUMNAS CATEGÓRICAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18bb17ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VALORES ÚNICOS EN COLUMNAS CATEGÓRICAS ===\n",
      "\n",
      "MARKET_TYPE:\n",
      "+-----------+\n",
      "|market_type|\n",
      "+-----------+\n",
      "|stock      |\n",
      "|crypto     |\n",
      "+-----------+\n",
      "\n",
      "CURRENCY:\n",
      "+--------+\n",
      "|currency|\n",
      "+--------+\n",
      "|usd     |\n",
      "|USDT    |\n",
      "|$       |\n",
      "|USD     |\n",
      "+--------+\n",
      "\n",
      "SECTOR:\n",
      "+-----------------------+\n",
      "|sector                 |\n",
      "+-----------------------+\n",
      "|automotive             |\n",
      "|finance                |\n",
      "|crypto                 |\n",
      "|Crypto                 |\n",
      "|Finance                |\n",
      "|blockchain             |\n",
      "|AI                     |\n",
      "|Automotive             |\n",
      "|RETAIL                 |\n",
      "|Technology             |\n",
      "|TECHNOLOGY             |\n",
      "|Artificial Intelligence|\n",
      "|ai                     |\n",
      "|Retail                 |\n",
      "|Blockchain             |\n",
      "|tech                   |\n",
      "+-----------------------+\n",
      "\n",
      "EXCHANGE:\n",
      "+--------+\n",
      "|exchange|\n",
      "+--------+\n",
      "|NYSE    |\n",
      "|Unknown |\n",
      "|N/A     |\n",
      "|COINBASE|\n",
      "|NASDAQ  |\n",
      "|BINANCE |\n",
      "|NULL    |\n",
      "+--------+\n",
      "\n",
      "COUNTRY:\n",
      "+-------+\n",
      "|country|\n",
      "+-------+\n",
      "|Global |\n",
      "|US     |\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== VALORES ÚNICOS EN COLUMNAS CATEGÓRICAS ===\\n\")\n",
    "\n",
    "cols_categoricas = ['market_type', 'currency', 'sector', 'exchange', 'country']\n",
    "\n",
    "for c in cols_categoricas:\n",
    "    print(f\"{c.upper()}:\")\n",
    "    df_raw.select(c).distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33ad48f",
   "metadata": {},
   "source": [
    "### FORMATOS DE FECHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92cf0937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FORMATOS DE FECHA ===\n",
      "\n",
      "+----------+\n",
      "|date      |\n",
      "+----------+\n",
      "|11-27-2023|\n",
      "|2024-07-14|\n",
      "|30/07/2024|\n",
      "|2023-05-18|\n",
      "|2024-08-20|\n",
      "|02/04/2024|\n",
      "|2024-01-19|\n",
      "|06-02-2023|\n",
      "|02-28-2024|\n",
      "|05-28-2023|\n",
      "|2023-05-01|\n",
      "|2023-01-21|\n",
      "|2023-04-17|\n",
      "|13/06/2023|\n",
      "|2024-08-06|\n",
      "|12/01/2023|\n",
      "|15/12/2023|\n",
      "|09-03-2023|\n",
      "|07-28-2024|\n",
      "|03-26-2024|\n",
      "+----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"=== FORMATOS DE FECHA ===\\n\")\n",
    "df_raw.select(\"date\").distinct().show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0703df41",
   "metadata": {},
   "source": [
    "### VALORES CON TEXTO EN 'open'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c601a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VALORES CON TEXTO EN 'open' ===\n",
      "\n",
      "+------------+\n",
      "|open        |\n",
      "+------------+\n",
      "|22984.61 USD|\n",
      "|2690.88 USD |\n",
      "|39825.2 USD |\n",
      "|37134.57 USD|\n",
      "|34251.52 USD|\n",
      "|8449.76 USD |\n",
      "|1542.32 USD |\n",
      "|20672.99 USD|\n",
      "|41460.8 USD |\n",
      "|25948.16 USD|\n",
      "|21806.06 USD|\n",
      "|21352.53 USD|\n",
      "|9231.11 USD |\n",
      "|4560.91 USD |\n",
      "|41361.06 USD|\n",
      "|29476.27 USD|\n",
      "|13562.88 USD|\n",
      "|20863.06 USD|\n",
      "|36202.99 USD|\n",
      "|9797.23 USD |\n",
      "+------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"=== VALORES CON TEXTO EN 'open' ===\\n\")\n",
    "df_raw.filter(col(\"open\").rlike(\"[a-zA-Z]\")).select(\"open\").distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc153091",
   "metadata": {},
   "source": [
    "### DUPLICADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8fd79fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DUPLICADOS ===\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o34.count.\n: org.apache.spark.SparkException: [INTERNAL_ERROR] The \"count\" action failed. You hit a bug in Spark or the Spark plugins you use. Please, report this bug to the corresponding communities or vendors, and provide the full stack trace. SQLSTATE: XX000\r\n\tat org.apache.spark.SparkException$.internalError(SparkException.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.toInternalError(QueryExecution.scala:643)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:656)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\r\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.classic.Dataset.count(Dataset.scala:1499)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor72.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\nCaused by: java.lang.NullPointerException: Cannot invoke \"org.apache.spark.sql.classic.SparkSession.sparkContext()\" because the return value of \"org.apache.spark.sql.execution.SparkPlan.session()\" is null\r\n\tat org.apache.spark.sql.execution.SparkPlan.sparkContext(SparkPlan.scala:68)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.metrics$lzycompute(HashAggregateExec.scala:71)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.metrics(HashAggregateExec.scala:70)\r\n\tat org.apache.spark.sql.execution.SparkPlan.resetMetrics(SparkPlan.scala:147)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.resetMetrics(AdaptiveSparkPlanExec.scala:245)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2233)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\t... 27 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== DUPLICADOS ===\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m total_filas = \u001b[43mdf_raw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m filas_unicas = df_raw.dropDuplicates().count()\n\u001b[32m      4\u001b[39m duplicados = total_filas - filas_unicas\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\polba\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:439\u001b[39m, in \u001b[36mDataFrame.count\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\polba\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\polba\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\polba\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o34.count.\n: org.apache.spark.SparkException: [INTERNAL_ERROR] The \"count\" action failed. You hit a bug in Spark or the Spark plugins you use. Please, report this bug to the corresponding communities or vendors, and provide the full stack trace. SQLSTATE: XX000\r\n\tat org.apache.spark.SparkException$.internalError(SparkException.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.toInternalError(QueryExecution.scala:643)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:656)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\r\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.classic.Dataset.count(Dataset.scala:1499)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor72.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\nCaused by: java.lang.NullPointerException: Cannot invoke \"org.apache.spark.sql.classic.SparkSession.sparkContext()\" because the return value of \"org.apache.spark.sql.execution.SparkPlan.session()\" is null\r\n\tat org.apache.spark.sql.execution.SparkPlan.sparkContext(SparkPlan.scala:68)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.metrics$lzycompute(HashAggregateExec.scala:71)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.metrics(HashAggregateExec.scala:70)\r\n\tat org.apache.spark.sql.execution.SparkPlan.resetMetrics(SparkPlan.scala:147)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.resetMetrics(AdaptiveSparkPlanExec.scala:245)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2233)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\t... 27 more\r\n"
     ]
    }
   ],
   "source": [
    "print(\"=== DUPLICADOS ===\\n\")\n",
    "total_filas = df_raw.count()\n",
    "filas_unicas = df_raw.dropDuplicates().count()\n",
    "duplicados = total_filas - filas_unicas\n",
    "print(f\"Filas duplicadas: {duplicados}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444d2c73",
   "metadata": {},
   "source": [
    "### VALORES ÚNICOS POR COLUMNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53de7386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VALORES ÚNICOS POR COLUMNA ===\n",
      "\n",
      "market_type: 2 valores únicos\n",
      "symbol: 11 valores únicos\n",
      "name: 10 valores únicos\n",
      "date: 1798 valores únicos\n",
      "open: 10000 valores únicos\n",
      "close: 10000 valores únicos\n",
      "high: 10000 valores únicos\n",
      "low: 10000 valores únicos\n",
      "volume: 10000 valores únicos\n",
      "market_cap: 10000 valores únicos\n",
      "sector: 16 valores únicos\n",
      "exchange: 7 valores únicos\n",
      "country: 2 valores únicos\n",
      "currency: 4 valores únicos\n",
      "last_updated: 7259 valores únicos\n"
     ]
    }
   ],
   "source": [
    "print(\"=== VALORES ÚNICOS POR COLUMNA ===\\n\")\n",
    "\n",
    "for col_name in df_raw.columns:\n",
    "    unique_count = df_raw.select(col_name).distinct().count()\n",
    "    print(f\"{col_name}: {unique_count} valores únicos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7417f877",
   "metadata": {},
   "source": [
    "### ESPACIOS EN BLANCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5148778e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ESPACIOS EN BLANCO ===\n",
      "\n",
      "market_type: 0 valores con espacios al inicio/final\n",
      "symbol: 0 valores con espacios al inicio/final\n",
      "name: 0 valores con espacios al inicio/final\n",
      "date: 0 valores con espacios al inicio/final\n",
      "open: 0 valores con espacios al inicio/final\n",
      "close: 0 valores con espacios al inicio/final\n",
      "high: 0 valores con espacios al inicio/final\n",
      "low: 0 valores con espacios al inicio/final\n",
      "volume: 0 valores con espacios al inicio/final\n",
      "market_cap: 0 valores con espacios al inicio/final\n",
      "sector: 0 valores con espacios al inicio/final\n",
      "exchange: 0 valores con espacios al inicio/final\n",
      "country: 0 valores con espacios al inicio/final\n",
      "currency: 0 valores con espacios al inicio/final\n",
      "last_updated: 0 valores con espacios al inicio/final\n"
     ]
    }
   ],
   "source": [
    "print(\"=== ESPACIOS EN BLANCO ===\\n\")\n",
    "\n",
    "for col_name in df_raw.columns:\n",
    "    con_espacios = df_raw.filter(trim(col(col_name)) != col(col_name)).count()\n",
    "    if con_espacios >= 0:\n",
    "        print(f\"{col_name}: {con_espacios} valores con espacios al inicio/final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ab4e2d",
   "metadata": {},
   "source": [
    "## TRANSFORM: Limpieza de datos\n",
    "\n",
    "### Paso 1: Crear copia de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "824616f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DataFrame listo para transformar: 10000 filas\n"
     ]
    }
   ],
   "source": [
    "df_clean = df_raw\n",
    "print(f\"✅ DataFrame listo para transformar: {df_clean.count()} filas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc66bd9",
   "metadata": {},
   "source": [
    "### Paso 2: Limpiar columna 'open' (extraer números)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90067168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes:\n",
      "+------------------+\n",
      "|open              |\n",
      "+------------------+\n",
      "|47536.20817743171 |\n",
      "|43310.14552728901 |\n",
      "|10624.832142807025|\n",
      "|14568.544718500116|\n",
      "|39260.94631003675 |\n",
      "+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Después:\n",
      "+------------------+\n",
      "|open              |\n",
      "+------------------+\n",
      "|47536.20817743171 |\n",
      "|43310.14552728901 |\n",
      "|10624.832142807025|\n",
      "|14568.544718500116|\n",
      "|39260.94631003675 |\n",
      "+------------------+\n",
      "only showing top 5 rows\n",
      "✅ 'open' limpiado\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "print(\"Antes:\")\n",
    "df_clean.select(\"open\").show(5, truncate=False)\n",
    "\n",
    "# Extraer solo números (patrón: dígitos con punto decimal opcional)\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"open\",\n",
    "    regexp_extract(col(\"open\"), r\"([\\d.]+)\", 1).cast(\"double\")\n",
    ")\n",
    "\n",
    "print(\"\\nDespués:\")\n",
    "df_clean.select(\"open\").show(5, truncate=False)\n",
    "print(\"✅ 'open' limpiado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e2b81c",
   "metadata": {},
   "source": [
    "### Paso 3: Normalizar columna 'currency'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "045ffd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes:\n",
      "+--------+\n",
      "|currency|\n",
      "+--------+\n",
      "|     usd|\n",
      "|    USDT|\n",
      "|       $|\n",
      "|     USD|\n",
      "+--------+\n",
      "\n",
      "Después:\n",
      "+--------+\n",
      "|currency|\n",
      "+--------+\n",
      "|     USD|\n",
      "+--------+\n",
      "\n",
      "✅ 'currency' normalizado\n"
     ]
    }
   ],
   "source": [
    "print(\"Antes:\")\n",
    "df_clean.select(\"currency\").distinct().show()\n",
    "\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"currency\",\n",
    "    when(upper(col(\"currency\")).isin([\"USD\", \"USDT\", \"$\"]), \"USD\")\n",
    "    .when(upper(col(\"currency\")) == \"USD\", \"USD\")\n",
    "    .otherwise(upper(col(\"currency\")))\n",
    ")\n",
    "\n",
    "print(\"Después:\")\n",
    "df_clean.select(\"currency\").distinct().show()\n",
    "print(\"✅ 'currency' normalizado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bd5fe6",
   "metadata": {},
   "source": [
    "### Paso 4: Normalizar columna 'sector'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92f1e48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes:\n",
      "+-----------------------+\n",
      "|sector                 |\n",
      "+-----------------------+\n",
      "|automotive             |\n",
      "|finance                |\n",
      "|crypto                 |\n",
      "|Crypto                 |\n",
      "|Finance                |\n",
      "|blockchain             |\n",
      "|AI                     |\n",
      "|Automotive             |\n",
      "|RETAIL                 |\n",
      "|Technology             |\n",
      "|TECHNOLOGY             |\n",
      "|Artificial Intelligence|\n",
      "|ai                     |\n",
      "|Retail                 |\n",
      "|Blockchain             |\n",
      "|tech                   |\n",
      "+-----------------------+\n",
      "\n",
      "Después:\n",
      "+----------+\n",
      "|    sector|\n",
      "+----------+\n",
      "|    Crypto|\n",
      "|   Finance|\n",
      "|        AI|\n",
      "|Automotive|\n",
      "|Technology|\n",
      "|    Retail|\n",
      "+----------+\n",
      "\n",
      "✅ 'sector' normalizado\n"
     ]
    }
   ],
   "source": [
    "print(\"Antes:\")\n",
    "df_clean.select(\"sector\").distinct().show(20, truncate=False)\n",
    "\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"sector\",\n",
    "    when(lower(col(\"sector\")).isin([\"finance\"]), \"Finance\")\n",
    "    .when(lower(col(\"sector\")).isin([\"tech\", \"technology\"]), \"Technology\")\n",
    "    .when(lower(col(\"sector\")).isin([\"retail\"]), \"Retail\")\n",
    "    .when(lower(col(\"sector\")).isin([\"crypto\", \"blockchain\"]), \"Crypto\")\n",
    "    .when(lower(col(\"sector\")).isin([\"ai\", \"artificial intelligence\"]), \"AI\")\n",
    "    .when(lower(col(\"sector\")).isin([\"automotive\"]), \"Automotive\")\n",
    "    .otherwise(col(\"sector\"))\n",
    ")\n",
    "\n",
    "print(\"Después:\")\n",
    "df_clean.select(\"sector\").distinct().show()\n",
    "print(\"✅ 'sector' normalizado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3f828e",
   "metadata": {},
   "source": [
    "### Paso 5: Limpiar columna 'exchange'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "159d4853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes:\n",
      "+--------+\n",
      "|exchange|\n",
      "+--------+\n",
      "|    NYSE|\n",
      "| Unknown|\n",
      "|     N/A|\n",
      "|COINBASE|\n",
      "|  NASDAQ|\n",
      "| BINANCE|\n",
      "|    NULL|\n",
      "+--------+\n",
      "\n",
      "\n",
      "Después:\n",
      "+--------------+\n",
      "|      exchange|\n",
      "+--------------+\n",
      "|          NYSE|\n",
      "|exchange_empty|\n",
      "|      COINBASE|\n",
      "|        NASDAQ|\n",
      "|       BINANCE|\n",
      "+--------------+\n",
      "\n",
      "✅ 'exchange' limpiado\n"
     ]
    }
   ],
   "source": [
    "print(\"Antes:\")\n",
    "df_clean.select(\"exchange\").distinct().show()\n",
    "\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"exchange\",\n",
    "    when(\n",
    "        col(\"exchange\").isNull() | \n",
    "        (col(\"exchange\") == \"Unknown\") | \n",
    "        (col(\"exchange\") == \"N/A\") | \n",
    "        (col(\"exchange\") == \"NULL\"),\n",
    "        \"exchange_empty\"\n",
    "    ).otherwise(col(\"exchange\"))\n",
    ")\n",
    "\n",
    "print(\"\\nDespués:\")\n",
    "df_clean.select(\"exchange\").distinct().show()\n",
    "print(\"✅ 'exchange' limpiado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c715b3ca",
   "metadata": {},
   "source": [
    "### Paso 6: Limpiar columna 'symbol' (rellenar según name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2475c2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulos en 'symbol': 213\n",
      "Nulos después: 0\n",
      "'symbol_empty' restantes: 0\n",
      "✅ 'symbol' limpiado\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first\n",
    "\n",
    "print(f\"Nulos en 'symbol': {df_clean.filter(col('symbol').isNull()).count()}\")\n",
    "\n",
    "# Crear mapeo name → symbol (de filas que sí tienen symbol)\n",
    "name_symbol_map = df_clean.filter(col(\"symbol\").isNotNull()) \\\n",
    "    .groupBy(\"name\") \\\n",
    "    .agg(first(\"symbol\").alias(\"symbol_map\"))\n",
    "\n",
    "# Join para rellenar nulos\n",
    "df_clean = df_clean.join(name_symbol_map, on=\"name\", how=\"left\")\n",
    "\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"symbol\",\n",
    "    when(col(\"symbol\").isNull(), col(\"symbol_map\"))\n",
    "    .otherwise(col(\"symbol\"))\n",
    ").drop(\"symbol_map\")\n",
    "\n",
    "# Si aún quedan nulos, rellenar con 'symbol_empty'\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"symbol\",\n",
    "    when(col(\"symbol\").isNull(), \"symbol_empty\")\n",
    "    .otherwise(col(\"symbol\"))\n",
    ")\n",
    "\n",
    "print(f\"Nulos después: {df_clean.filter(col('symbol').isNull()).count()}\")\n",
    "print(f\"'symbol_empty' restantes: {df_clean.filter(col('symbol') == 'symbol_empty').count()}\")\n",
    "print(\"✅ 'symbol' limpiado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76463984",
   "metadata": {},
   "source": [
    "### Paso 7: Normalizar columna 'date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55d28364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2024-08-03|\n",
      "|2023-06-13|\n",
      "|2024-08-13|\n",
      "|2024-08-15|\n",
      "|2024-03-29|\n",
      "|2024-03-13|\n",
      "|2023-10-27|\n",
      "|2023-08-24|\n",
      "|2023-07-22|\n",
      "|2023-04-25|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "Nulos: 0\n"
     ]
    }
   ],
   "source": [
    "df_clean = df_clean.withColumn(\n",
    "    \"date\",\n",
    "    when(\n",
    "        col(\"date\").rlike(r\"^\\d{4}-\\d{2}-\\d{2}$\"),  # YYYY-MM-DD\n",
    "        make_date(\n",
    "            split(col(\"date\"), \"-\")[0].cast(\"int\"),\n",
    "            split(col(\"date\"), \"-\")[1].cast(\"int\"),\n",
    "            split(col(\"date\"), \"-\")[2].cast(\"int\")\n",
    "        )\n",
    "    ).when(\n",
    "        col(\"date\").rlike(r\"^\\d{2}/\\d{2}/\\d{4}$\"),  # DD/MM/YYYY\n",
    "        make_date(\n",
    "            split(col(\"date\"), \"/\")[2].cast(\"int\"),\n",
    "            split(col(\"date\"), \"/\")[1].cast(\"int\"),\n",
    "            split(col(\"date\"), \"/\")[0].cast(\"int\")\n",
    "        )\n",
    "    ).when(\n",
    "        col(\"date\").rlike(r\"^\\d{2}-\\d{2}-\\d{4}$\"),  # MM-DD-YYYY\n",
    "        make_date(\n",
    "            split(col(\"date\"), \"-\")[2].cast(\"int\"),\n",
    "            split(col(\"date\"), \"-\")[0].cast(\"int\"),\n",
    "            split(col(\"date\"), \"-\")[1].cast(\"int\")\n",
    "        )\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "df_clean.select(\"date\").show(10)\n",
    "print(f\"Nulos: {df_clean.filter(col('date').isNull()).count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1274d964",
   "metadata": {},
   "source": [
    "### Paso 8: Convertir 'last_updated' a timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2a07967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipo antes: StringType()\n",
      "Tipo después: TimestampType()\n",
      "✅ 'last_updated' convertido\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tipo antes: {df_clean.schema['last_updated'].dataType}\")\n",
    "\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"last_updated\",\n",
    "    to_timestamp(col(\"last_updated\"), \"yyyy-MM-dd HH:mm:ss\")\n",
    ")\n",
    "\n",
    "print(f\"Tipo después: {df_clean.schema['last_updated'].dataType}\")\n",
    "print(\"✅ 'last_updated' convertido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b396d1",
   "metadata": {},
   "source": [
    "### Paso 9: Crear columnas derivadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1c778f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Columnas derivadas creadas\n",
      "+------------------+------------------+-------------------+------------------+\n",
      "|              open|             close|       daily_change|  daily_change_pct|\n",
      "+------------------+------------------+-------------------+------------------+\n",
      "| 47536.20817743171|49741.830640201704|  2205.622462769992| 4.639878836228114|\n",
      "| 43310.14552728901| 44186.00670200739|  875.8611747183822|2.0223002348641765|\n",
      "|10624.832142807025|  9948.72087971569|  -676.111263091334|-6.363500655857974|\n",
      "|14568.544718500116| 14894.45149823145|  325.9067797313346|2.2370578944475925|\n",
      "| 39260.94631003675|36902.728007201345|-2358.2183028354048|-6.006524356832797|\n",
      "+------------------+------------------+-------------------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df_clean = df_clean.withColumn(\n",
    "    \"daily_change\",\n",
    "    col(\"close\") - col(\"open\")\n",
    ")\n",
    "\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"daily_change_pct\",\n",
    "    ((col(\"close\") - col(\"open\")) / col(\"open\")) * 100\n",
    ")\n",
    "\n",
    "print(\"✅ Columnas derivadas creadas\")\n",
    "df_clean.select(\"open\", \"close\", \"daily_change\", \"daily_change_pct\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c11a9b4",
   "metadata": {},
   "source": [
    "### Paso 10: Reducir decimales a 2 dígitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccbd3e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Decimales reducidos a 2 dígitos\n",
      "+--------+--------+--------+--------+----------+-------------------+------------+----------------+\n",
      "|    open|   close|    high|     low|    volume|         market_cap|daily_change|daily_change_pct|\n",
      "+--------+--------+--------+--------+----------+-------------------+------------+----------------+\n",
      "|47536.21|49741.83|51230.75|45530.22|1560789.21|1.12987495535361E12|     2205.62|            4.64|\n",
      "|43310.15|44186.01|45750.35|41189.21|9699128.61| 1.8367000038292E11|      875.86|            2.02|\n",
      "|10624.83| 9948.72|10722.26| 9602.63|5248039.56| 3.3381484311617E11|     -676.11|           -6.36|\n",
      "|14568.54|14894.45|14998.34|14052.92|3664252.07| 1.7561828236364E11|      325.91|            2.24|\n",
      "|39260.95|36902.73|40270.41|36150.68| 465457.68| 1.8514764376573E11|    -2358.22|           -6.01|\n",
      "+--------+--------+--------+--------+----------+-------------------+------------+----------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "cols_decimales = ['open', 'close', 'high', 'low', 'volume', 'market_cap', 'daily_change', 'daily_change_pct']\n",
    "\n",
    "for c in cols_decimales:\n",
    "    df_clean = df_clean.withColumn(c, round(col(c), 2))\n",
    "\n",
    "print(\"✅ Decimales reducidos a 2 dígitos\")\n",
    "df_clean.select(cols_decimales).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748fac88",
   "metadata": {},
   "source": [
    "# LOAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570e8f7a",
   "metadata": {},
   "source": [
    "### Guardar CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a26b41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV guardado en ../data/market_data_clean_spark.csv\n"
     ]
    }
   ],
   "source": [
    "# Uso pandas para guardar porque no he podido con Spark directamente\n",
    "df_clean.toPandas().to_csv(\"../data/market_data_clean_spark.csv\", index=False)\n",
    "print(\"✅ CSV guardado en ../data/market_data_clean_spark.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0cdb8a",
   "metadata": {},
   "source": [
    "### Limpiar sesion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b943d332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sesión cerrada\n"
     ]
    }
   ],
   "source": [
    "df_clean.unpersist()\n",
    "spark.stop()\n",
    "print(\"✅ Sesión cerrada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb17a9f",
   "metadata": {},
   "source": [
    "## Iniciar sesion y cargar CSV limpio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae030456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------+----------+--------+--------+--------+--------+----------+-------------------+-------+--------+-------+--------+-------------------+------------+----------------+\n",
      "|     name|market_type|symbol|      date|    open|   close|    high|     low|    volume|         market_cap| sector|exchange|country|currency|       last_updated|daily_change|daily_change_pct|\n",
      "+---------+-----------+------+----------+--------+--------+--------+--------+----------+-------------------+-------+--------+-------+--------+-------------------+------------+----------------+\n",
      "| MSFT Inc|      stock|  MSFT|2024-08-03|47536.21|49741.83|51230.75|45530.22|1560789.21|1.12987495535361E12|Finance|  NASDAQ|     US|     USD|2024-08-03 23:00:00|     2205.62|            4.64|\n",
      "|GOOGL Inc|      stock| GOOGL|2023-06-13|43310.15|44186.01|45750.35|41189.21|9699128.61| 1.8367000038292E11| Retail|  NASDAQ| Global|     USD|2023-06-13 12:00:00|      875.86|            2.02|\n",
      "| BTC Coin|     crypto|   BTC|2024-08-13|10624.83| 9948.72|10722.26| 9602.63|5248039.56| 3.3381484311617E11| Crypto|COINBASE|     US|     USD|2024-08-13 02:00:00|     -676.11|           -6.36|\n",
      "| SOL Coin|     crypto|   SOL|2024-08-15|14568.54|14894.45|14998.34|14052.92|3664252.07| 1.7561828236364E11| Crypto| BINANCE|     US|     USD|2024-08-15 22:00:00|      325.91|            2.24|\n",
      "| SOL Coin|     crypto|   SOL|2024-03-29|39260.95|36902.73|40270.41|36150.68| 465457.68| 1.8514764376573E11| Crypto| BINANCE| Global|     USD|2024-03-29 19:00:00|    -2358.22|           -6.01|\n",
      "+---------+-----------+------+----------+--------+--------+--------+--------+----------+-------------------+-------+--------+-------+--------+-------------------+------------+----------------+\n",
      "only showing top 5 rows\n",
      "Filas: 10000, Columnas: 17\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ETL Market Data\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"../data/market_data_clean_spark.csv\", header=True, inferSchema=True)\n",
    "df.cache()\n",
    "df.show(5)\n",
    "print(f\"Filas: {df.count()}, Columnas: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fcb404",
   "metadata": {},
   "source": [
    "## Crear tablas dimensionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1561fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-----------+--------+\n",
      "|symbol|     name|market_type|asset_id|\n",
      "+------+---------+-----------+--------+\n",
      "|  AAPL| AAPL Inc|      stock|       1|\n",
      "|   ETH| ETH Coin|     crypto|       2|\n",
      "|  MSFT| MSFT Inc|      stock|       3|\n",
      "|  AMZN| AMZN Inc|      stock|       4|\n",
      "| GOOGL|GOOGL Inc|      stock|       5|\n",
      "|   SOL| SOL Coin|     crypto|       6|\n",
      "|   ADA| ADA Coin|     crypto|       7|\n",
      "|   BTC| BTC Coin|     crypto|       8|\n",
      "|   BNB| BNB Coin|     crypto|       9|\n",
      "|  TSLA| TSLA Inc|      stock|      10|\n",
      "+------+---------+-----------+--------+\n",
      "\n",
      "+-----------+---------+\n",
      "|sector_name|sector_id|\n",
      "+-----------+---------+\n",
      "|     Crypto|        1|\n",
      "|    Finance|        2|\n",
      "|         AI|        3|\n",
      "| Automotive|        4|\n",
      "| Technology|        5|\n",
      "|     Retail|        6|\n",
      "+-----------+---------+\n",
      "\n",
      "+--------------+-----------+\n",
      "| exchange_name|exchange_id|\n",
      "+--------------+-----------+\n",
      "|          NYSE|          1|\n",
      "|exchange_empty|          2|\n",
      "|      COINBASE|          3|\n",
      "|        NASDAQ|          4|\n",
      "|       BINANCE|          5|\n",
      "+--------------+-----------+\n",
      "\n",
      "+-------------+-----------+\n",
      "|currency_name|currency_id|\n",
      "+-------------+-----------+\n",
      "|          USD|          1|\n",
      "+-------------+-----------+\n",
      "\n",
      "+------------+----------+\n",
      "|country_name|country_id|\n",
      "+------------+----------+\n",
      "|      Global|         1|\n",
      "|          US|         2|\n",
      "+------------+----------+\n",
      "\n",
      "+----------+-------+----+-----+---+-------+\n",
      "|      date|date_id|year|month|day|weekday|\n",
      "+----------+-------+----+-----+---+-------+\n",
      "|2023-07-15|      1|2023|    7| 15|      7|\n",
      "|2023-06-22|      2|2023|    6| 22|      5|\n",
      "|2024-05-30|      3|2024|    5| 30|      5|\n",
      "|2023-11-08|      4|2023|   11|  8|      4|\n",
      "|2024-06-12|      5|2024|    6| 12|      4|\n",
      "|2024-02-05|      6|2024|    2|  5|      2|\n",
      "|2023-05-22|      7|2023|    5| 22|      2|\n",
      "|2024-06-04|      8|2024|    6|  4|      3|\n",
      "|2023-09-14|      9|2023|    9| 14|      5|\n",
      "|2023-09-19|     10|2023|    9| 19|      3|\n",
      "|2023-06-18|     11|2023|    6| 18|      1|\n",
      "|2023-02-25|     12|2023|    2| 25|      7|\n",
      "|2023-11-22|     13|2023|   11| 22|      4|\n",
      "|2024-05-25|     14|2024|    5| 25|      7|\n",
      "|2024-01-07|     15|2024|    1|  7|      1|\n",
      "|2023-02-08|     16|2023|    2|  8|      4|\n",
      "|2023-06-23|     17|2023|    6| 23|      6|\n",
      "|2024-04-20|     18|2024|    4| 20|      7|\n",
      "|2023-11-29|     19|2023|   11| 29|      4|\n",
      "|2023-12-10|     20|2023|   12| 10|      1|\n",
      "+----------+-------+----+-----+---+-------+\n",
      "only showing top 20 rows\n",
      "✅ Dimensiones creadas\n"
     ]
    }
   ],
   "source": [
    "# 1. dim_asset\n",
    "dim_asset = df.select(\"symbol\", \"name\", \"market_type\").dropDuplicates()\n",
    "dim_asset = dim_asset.withColumn(\"asset_id\", monotonically_increasing_id() + 1)\n",
    "dim_asset.show()\n",
    "\n",
    "# 2. dim_sector\n",
    "dim_sector = df.select(\"sector\").dropDuplicates().withColumnRenamed(\"sector\", \"sector_name\")\n",
    "dim_sector = dim_sector.withColumn(\"sector_id\", monotonically_increasing_id() + 1)\n",
    "dim_sector.show()\n",
    "\n",
    "# 3. dim_exchange\n",
    "dim_exchange = df.select(\"exchange\").dropDuplicates().withColumnRenamed(\"exchange\", \"exchange_name\")\n",
    "dim_exchange = dim_exchange.withColumn(\"exchange_id\", monotonically_increasing_id() + 1)\n",
    "dim_exchange.show()\n",
    "\n",
    "# 4. dim_currency\n",
    "dim_currency = df.select(\"currency\").dropDuplicates().withColumnRenamed(\"currency\", \"currency_name\")\n",
    "dim_currency = dim_currency.withColumn(\"currency_id\", monotonically_increasing_id() + 1)\n",
    "dim_currency.show()\n",
    "\n",
    "# 5. dim_country\n",
    "dim_country = df.select(\"country\").dropDuplicates().withColumnRenamed(\"country\", \"country_name\")\n",
    "dim_country = dim_country.withColumn(\"country_id\", monotonically_increasing_id() + 1)\n",
    "dim_country.show()\n",
    "\n",
    "# 6. dim_date\n",
    "dim_date = df.select(\"date\").dropDuplicates()\n",
    "dim_date = dim_date.withColumn(\"date_id\", monotonically_increasing_id() + 1) \\\n",
    "    .withColumn(\"year\", year(\"date\")) \\\n",
    "    .withColumn(\"month\", month(\"date\")) \\\n",
    "    .withColumn(\"day\", dayofmonth(\"date\")) \\\n",
    "    .withColumn(\"weekday\", dayofweek(\"date\"))\n",
    "dim_date.show()\n",
    "\n",
    "print(\"✅ Dimensiones creadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906f95cb",
   "metadata": {},
   "source": [
    "## Crear tabla de hechos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d065395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+---------+-----------+-----------+----------+--------+--------+--------+--------+----------+-------------------+------------+----------------+\n",
      "|asset_id|date_id|sector_id|exchange_id|currency_id|country_id|    open|   close|    high|     low|    volume|         market_cap|daily_change|daily_change_pct|\n",
      "+--------+-------+---------+-----------+-----------+----------+--------+--------+--------+--------+----------+-------------------+------------+----------------+\n",
      "|       3|    107|        2|          4|          1|         2|47536.21|49741.83|51230.75|45530.22|1560789.21|1.12987495535361E12|     2205.62|            4.64|\n",
      "|       5|    574|        6|          4|          1|         1|43310.15|44186.01|45750.35|41189.21|9699128.61| 1.8367000038292E11|      875.86|            2.02|\n",
      "|       8|    330|        1|          3|          1|         2|10624.83| 9948.72|10722.26| 9602.63|5248039.56| 3.3381484311617E11|     -676.11|           -6.36|\n",
      "|       6|    108|        1|          5|          1|         2|14568.54|14894.45|14998.34|14052.92|3664252.07| 1.7561828236364E11|      325.91|            2.24|\n",
      "|       6|    193|        1|          5|          1|         1|39260.95|36902.73|40270.41|36150.68| 465457.68| 1.8514764376573E11|    -2358.22|           -6.01|\n",
      "+--------+-------+---------+-----------+-----------+----------+--------+--------+--------+--------+----------+-------------------+------------+----------------+\n",
      "only showing top 5 rows\n",
      "✅ Fact table creada: 10000 filas\n"
     ]
    }
   ],
   "source": [
    "# Crear fact_market_data con joins a las dimensiones\n",
    "fact = df.alias(\"f\") \\\n",
    "    .join(dim_asset.alias(\"a\"), (col(\"f.symbol\") == col(\"a.symbol\")) & (col(\"f.name\") == col(\"a.name\")), \"left\") \\\n",
    "    .join(dim_sector.alias(\"s\"), col(\"f.sector\") == col(\"s.sector_name\"), \"left\") \\\n",
    "    .join(dim_exchange.alias(\"e\"), col(\"f.exchange\") == col(\"e.exchange_name\"), \"left\") \\\n",
    "    .join(dim_currency.alias(\"c\"), col(\"f.currency\") == col(\"c.currency_name\"), \"left\") \\\n",
    "    .join(dim_country.alias(\"co\"), col(\"f.country\") == col(\"co.country_name\"), \"left\") \\\n",
    "    .join(dim_date.alias(\"d\"), col(\"f.date\") == col(\"d.date\"), \"left\") \\\n",
    "    .select(\n",
    "        col(\"a.asset_id\"),\n",
    "        col(\"d.date_id\"),\n",
    "        col(\"s.sector_id\"),\n",
    "        col(\"e.exchange_id\"),\n",
    "        col(\"c.currency_id\"),\n",
    "        col(\"co.country_id\"),\n",
    "        col(\"f.open\"),\n",
    "        col(\"f.close\"),\n",
    "        col(\"f.high\"),\n",
    "        col(\"f.low\"),\n",
    "        col(\"f.volume\"),\n",
    "        col(\"f.market_cap\"),\n",
    "        col(\"f.daily_change\"),\n",
    "        col(\"f.daily_change_pct\")\n",
    "    )\n",
    "\n",
    "fact.show(5)\n",
    "print(f\"✅ Fact table creada: {fact.count()} filas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3fd784",
   "metadata": {},
   "source": [
    "## Guardar en SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44f55afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Warehouse guardado en ../warehouse/warehouse_pyspark.db\n",
      "   • 6 dimensiones\n",
      "   • 1 fact table: 10000 filas\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Convertir a Pandas\n",
    "dim_asset_pd = dim_asset.toPandas()\n",
    "dim_sector_pd = dim_sector.toPandas()\n",
    "dim_exchange_pd = dim_exchange.toPandas()\n",
    "dim_currency_pd = dim_currency.toPandas()\n",
    "dim_country_pd = dim_country.toPandas()\n",
    "dim_date_pd = dim_date.toPandas()\n",
    "fact_pd = fact.toPandas()\n",
    "\n",
    "# Conexión SQLite\n",
    "engine = create_engine('sqlite:///../warehouse/warehouse_pyspark.db')\n",
    "\n",
    "# Guardar tablas\n",
    "dim_asset_pd.to_sql('dim_asset', engine, if_exists='replace', index=False)\n",
    "dim_sector_pd.to_sql('dim_sector', engine, if_exists='replace', index=False)\n",
    "dim_exchange_pd.to_sql('dim_exchange', engine, if_exists='replace', index=False)\n",
    "dim_currency_pd.to_sql('dim_currency', engine, if_exists='replace', index=False)\n",
    "dim_country_pd.to_sql('dim_country', engine, if_exists='replace', index=False)\n",
    "dim_date_pd.to_sql('dim_date', engine, if_exists='replace', index=False)\n",
    "fact_pd.to_sql('fact_market_data', engine, if_exists='replace', index=False)\n",
    "\n",
    "print(\"✅ Warehouse guardado en ../warehouse/warehouse_pyspark.db\")\n",
    "print(f\"   • 6 dimensiones\")\n",
    "print(f\"   • 1 fact table: {len(fact_pd)} filas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0568fb44",
   "metadata": {},
   "source": [
    "## Generar DDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b31072d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DDL guardado\n",
      "-- DDL Datawarehouse Market Data (PySpark)\n",
      "\n",
      "\n",
      "CREATE TABLE dim_asset (\n",
      "    symbol TEXT,\n",
      "    name TEXT,\n",
      "    market_type TEXT,\n",
      "    asset_id INTEGER PRIMARY KEY\n",
      ");\n",
      "\n",
      "CREATE TABLE dim_sector (\n",
      "    sector_name TEXT,\n",
      "    sector_id INTEGER PRIMARY KEY\n",
      ");\n",
      "\n",
      "CREATE TABLE dim_exchange (\n",
      "    exchange_name TEXT,\n",
      "    exchange_id INTEGER PRIMARY KEY\n",
      ");\n",
      "\n",
      "CREATE TABLE dim_currency (\n",
      "    currency_name TEXT,\n",
      "    currency_id INTEGER PRIMARY KEY\n",
      ");\n",
      "\n",
      "CREATE TABLE dim_country (\n",
      "    country_name TEXT,\n",
      "    country_id INTEGER PRIMARY KEY\n",
      ");\n",
      "\n",
      "CREATE TABLE dim_date (\n",
      "    date DATE,\n",
      "    date_id INTEGER PRIMARY KEY,\n",
      "    year INTEGER,\n",
      "    month INTEGER,\n",
      "    day INTEGER,\n",
      "    weekday INTEGER\n",
      ");\n",
      "\n",
      "CREATE TABLE fact_market_data (\n",
      "    asset_id INTEGER,\n",
      "    date_id INTEGER,\n",
      "    sector_id INTEGER,\n",
      "    exchange_id INTEGER,\n",
      "    currency_id INTEGER,\n",
      "    country_id INTEGER,\n",
      "    open REAL,\n",
      "    close REAL,\n",
      "    high REAL,\n",
      "    low REAL,\n",
      "    volume REAL,\n",
      "    market_cap REAL,\n",
      "    daily_change REAL,\n",
      "    daily_change_pct REAL,\n",
      "    FOREIGN KEY (asset_id) REFERENCES dim_asset(asset_id),\n",
      "    FOREIGN KEY (date_id) REFERENCES dim_date(date_id),\n",
      "    FOREIGN KEY (sector_id) REFERENCES dim_sector(sector_id),\n",
      "    FOREIGN KEY (exchange_id) REFERENCES dim_exchange(exchange_id),\n",
      "    FOREIGN KEY (currency_id) REFERENCES dim_currency(currency_id),\n",
      "    FOREIGN KEY (country_id) REFERENCES dim_country(country_id)\n",
      ");\n"
     ]
    }
   ],
   "source": [
    "def spark_dtype_to_sql(dtype):\n",
    "    dtype_str = str(dtype).lower()\n",
    "    if 'int' in dtype_str or 'long' in dtype_str:\n",
    "        return \"INTEGER\"\n",
    "    elif 'double' in dtype_str or 'float' in dtype_str:\n",
    "        return \"REAL\"\n",
    "    elif 'date' in dtype_str or 'timestamp' in dtype_str:\n",
    "        return \"DATE\"\n",
    "    elif 'boolean' in dtype_str:\n",
    "        return \"BOOLEAN\"\n",
    "    else:\n",
    "        return \"TEXT\"\n",
    "\n",
    "def generate_ddl(df, table_name, pk=None, fks=None):\n",
    "    lines = [f\"CREATE TABLE {table_name} (\"]\n",
    "    cols = []\n",
    "    for field in df.schema.fields:\n",
    "        col_def = f\"    {field.name} {spark_dtype_to_sql(field.dataType)}\"\n",
    "        if pk and field.name == pk:\n",
    "            col_def += \" PRIMARY KEY\"\n",
    "        cols.append(col_def)\n",
    "    if fks:\n",
    "        for fk_col, ref_table, ref_col in fks:\n",
    "            cols.append(f\"    FOREIGN KEY ({fk_col}) REFERENCES {ref_table}({ref_col})\")\n",
    "    lines.append(\",\\n\".join(cols))\n",
    "    lines.append(\");\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# Generar DDLs\n",
    "ddl_parts = [\"-- DDL Datawarehouse Market Data (PySpark)\\n\"]\n",
    "ddl_parts.append(generate_ddl(dim_asset, \"dim_asset\", pk=\"asset_id\"))\n",
    "ddl_parts.append(generate_ddl(dim_sector, \"dim_sector\", pk=\"sector_id\"))\n",
    "ddl_parts.append(generate_ddl(dim_exchange, \"dim_exchange\", pk=\"exchange_id\"))\n",
    "ddl_parts.append(generate_ddl(dim_currency, \"dim_currency\", pk=\"currency_id\"))\n",
    "ddl_parts.append(generate_ddl(dim_country, \"dim_country\", pk=\"country_id\"))\n",
    "ddl_parts.append(generate_ddl(dim_date, \"dim_date\", pk=\"date_id\"))\n",
    "ddl_parts.append(generate_ddl(fact, \"fact_market_data\", fks=[\n",
    "    (\"asset_id\", \"dim_asset\", \"asset_id\"),\n",
    "    (\"date_id\", \"dim_date\", \"date_id\"),\n",
    "    (\"sector_id\", \"dim_sector\", \"sector_id\"),\n",
    "    (\"exchange_id\", \"dim_exchange\", \"exchange_id\"),\n",
    "    (\"currency_id\", \"dim_currency\", \"currency_id\"),\n",
    "    (\"country_id\", \"dim_country\", \"country_id\")\n",
    "]))\n",
    "\n",
    "ddl_sql = \"\\n\\n\".join(ddl_parts)\n",
    "\n",
    "with open(\"../warehouse/modelo_datawarehouse_pyspark.sql\", \"w\") as f:\n",
    "    f.write(ddl_sql)\n",
    "\n",
    "print(\"✅ DDL guardado\")\n",
    "print(ddl_sql)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
