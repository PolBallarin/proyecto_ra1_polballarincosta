# Proyecto RA1 - Big Data: ETL y Datawarehouse para Datos de Mercado

**Autor:** Pol Ballarin Costa  
**Fecha:** Diciembre 2025

---

## üìã Descripci√≥n del Proyecto

Este proyecto implementa un proceso completo de ETL (Extract, Transform, Load) para limpiar y transformar datos de mercado financiero (acciones y criptomonedas), creando un Datawarehouse estructurado con modelo dimensional (Star Schema). El proyecto utiliza tanto **Pandas** como **PySpark** para procesar los datos, demostrando diferentes enfoques de procesamiento de datos.

### Dataset

El dataset contiene **10,000 registros** de datos de mercado con informaci√≥n de:
- **Acciones (stocks):** MSFT, GOOGL, TSLA, AMZN, AAPL
- **Criptomonedas (crypto):** BTC, ETH, SOL, ADA, BNB

### Objetivos

- Realizar exploraci√≥n y limpieza exhaustiva de datos de mercado financiero
- Implementar procesos ETL con Pandas y PySpark
- Crear un Datawarehouse con modelo dimensional (1 tabla de hechos + 6 tablas de dimensiones)
- Generar DDLs para la estructura del Datawarehouse
- Contenedorizar el entorno con Docker
- Documentar todo el proceso y resultados

---

## üóÇÔ∏è Estructura de Carpetas

```
proyecto_market_data/
‚îú‚îÄ‚îÄ data/                              # Datos del proyecto
‚îÇ   ‚îú‚îÄ‚îÄ market_data.csv                # Dataset original
‚îÇ   ‚îú‚îÄ‚îÄ market_data_clean.csv          # Dataset limpio (Pandas)
‚îÇ   ‚îî‚îÄ‚îÄ market_data_clean_spark.csv    # Dataset limpio (PySpark)
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                         # Notebooks Jupyter
‚îÇ   ‚îú‚îÄ‚îÄ 01_pandas.ipynb               # ETL con Pandas
‚îÇ   ‚îî‚îÄ‚îÄ 02_pyspark.ipynb              # ETL con PySpark
‚îÇ
‚îú‚îÄ‚îÄ warehouse/                         # Datawarehouse
‚îÇ   ‚îú‚îÄ‚îÄ warehouse_pandas.db           # Base de datos SQLite (Pandas)
‚îÇ   ‚îú‚îÄ‚îÄ warehouse_pyspark.db          # Base de datos SQLite (PySpark)
‚îÇ   ‚îú‚îÄ‚îÄ modelo_datawarehouse_pandas.sql    # DDL del modelo (Pandas)
‚îÇ   ‚îî‚îÄ‚îÄ modelo_datawarehouse_pyspark.sql   # DDL del modelo (PySpark)
‚îÇ
‚îú‚îÄ‚îÄ docs/                             # Documentaci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ README.md                     # Este archivo
‚îÇ   ‚îî‚îÄ‚îÄ diagrama.drawio               # Diagrama del modelo dimensional
‚îÇ
‚îú‚îÄ‚îÄ Dockerfile                        # Imagen Docker
‚îú‚îÄ‚îÄ docker-compose.yml                # Orquestaci√≥n de contenedores
‚îî‚îÄ‚îÄ requirements.txt                  # Dependencias Python
```

---

## üõ†Ô∏è Herramientas Utilizadas

### Lenguajes y Frameworks
- **Python 3.11**: Lenguaje de programaci√≥n principal
- **Pandas**: Biblioteca para manipulaci√≥n y an√°lisis de datos
- **PySpark 4.0.1**: Framework para procesamiento distribuido de datos
- **SQLite**: Base de datos relacional para el Datawarehouse
- **SQLAlchemy**: ORM para conexi√≥n con SQLite

### Herramientas de Desarrollo
- **Jupyter Notebook/Lab**: Entorno de desarrollo interactivo
- **Docker**: Contenedorizaci√≥n del entorno de desarrollo
- **Docker Compose**: Orquestaci√≥n de contenedores

### Librer√≠as Python
- `pandas`: Manipulaci√≥n de DataFrames
- `pyspark`: Procesamiento distribuido
- `numpy`: Operaciones num√©ricas
- `sqlalchemy`: Conexi√≥n a bases de datos
- `re`: Expresiones regulares para limpieza de datos

---

## üìä Problemas Detectados en el Dataset

Durante la fase de exploraci√≥n se identificaron los siguientes problemas de calidad:

| Columna | Problema | Soluci√≥n Aplicada |
|---------|----------|-------------------|
| `date` | 3 formatos diferentes: MM-DD-YYYY, YYYY-MM-DD, DD/MM/YYYY | Detecci√≥n con regex + parsing condicional |
| `open` | 513 valores con texto ("4892.63 USD") | Extracci√≥n de n√∫meros con regex |
| `currency` | 4 variantes: usd, USD, $, USDT | Normalizaci√≥n a "USD" |
| `sector` | 16 valores inconsistentes (tech/Tech/TECHNOLOGY, blockchain/Crypto) | Mapeo a 6 categor√≠as |
| `exchange` | 138 "Unknown" + 361 nulos | Reemplazo por "exchange_empty" |
| `symbol` | 213 nulos | Rellenado usando mapeo name‚Üísymbol |

---

## üìä Explicaci√≥n de Cada Fase

### Fase 1: Exploraci√≥n y Limpieza con Pandas (`01_pandas.ipynb`)

**Objetivo:** Explorar y limpiar el dataset usando Pandas.

**Proceso:**
1. **Carga de datos**: Lectura del CSV original
2. **An√°lisis exploratorio**: 
   - Detecci√≥n de 574 valores nulos totales
   - Identificaci√≥n de 0 duplicados
   - An√°lisis de valores √∫nicos por columna
3. **Limpieza de datos**:
   - Extracci√≥n de n√∫meros de columna `open`
   - Normalizaci√≥n de `currency` a "USD"
   - Mapeo de 16 sectores a 6 categor√≠as
   - Parsing de 3 formatos de fecha
   - Rellenado de `symbol` usando `name`
4. **Columnas derivadas**:
   - `daily_change`: cambio absoluto (close - open)
   - `daily_change_pct`: cambio porcentual

**Resultado:** Dataset limpio con 10,000 filas y 17 columnas

---

### Fase 2: Procesamiento con PySpark (`02_pyspark.ipynb`)

**Objetivo:** Replicar el proceso ETL usando PySpark.

**Diferencias con Pandas:**
- **Evaluaci√≥n lazy**: Las transformaciones no se ejecutan hasta una acci√≥n
- **Inmutabilidad**: Cada transformaci√≥n crea un nuevo DataFrame
- **Funciones nativas**: Uso de `when()`, `regexp_extract()`, `make_date()` en lugar de UDFs
- **Parseo de fechas**: Uso de `make_date()` con `split()` para evitar errores de formato

**Resultado:** Dataset limpio guardado en `market_data_clean_spark.csv`

---

### Fase 3: Modelo de Data Warehouse

**Modelo Star Schema con 1 tabla de hechos y 6 dimensiones:**

#### Tabla de Hechos: `fact_market_data`
- **M√©tricas:** open, close, high, low, volume, market_cap, daily_change, daily_change_pct
- **Foreign Keys:** asset_id, date_id, sector_id, exchange_id, currency_id, country_id

#### Tablas Dimensionales

| Dimensi√≥n | Descripci√≥n | Registros |
|-----------|-------------|-----------|
| `dim_asset` | Activos (symbol, name, market_type) | 10 |
| `dim_sector` | Sectores (Finance, Technology, Retail, Crypto, AI, Automotive) | 6 |
| `dim_exchange` | Exchanges (NASDAQ, NYSE, BINANCE, COINBASE, exchange_empty) | 5 |
| `dim_currency` | Monedas (USD) | 1 |
| `dim_country` | Pa√≠ses (US, Global) | 2 |
| `dim_date` | Fechas con componentes (year, month, day, weekday) | ~600 |

---

### Fase 4: Docker

**Archivos de configuraci√≥n:**

**Dockerfile:**
- Base: `python:3.11-slim`
- Instalaci√≥n de Java JDK (requerido para PySpark)
- Instalaci√≥n de dependencias desde `requirements.txt`
- Puerto 8888 para Jupyter

**docker-compose.yml:**
- Vol√∫menes montados: data, notebooks, docs, warehouse
- Variable de entorno para JupyterLab
- Reinicio autom√°tico del contenedor

---

## üöÄ Instrucciones de Ejecuci√≥n

### Opci√≥n 1: Con Docker (Recomendado)

1. **Navegar al directorio del proyecto:**
   ```bash
   cd proyecto_market_data
   ```

2. **Construir y ejecutar el contenedor:**
   ```bash
   docker-compose up --build
   ```

3. **Acceder a Jupyter:**
   - Abrir navegador en: `http://localhost:8888`

4. **Ejecutar los notebooks en orden:**
   - `01_pandas.ipynb`
   - `02_pyspark.ipynb`

### Opci√≥n 2: Sin Docker

1. **Instalar dependencias:**
   ```bash
   pip install jupyter pandas pyspark sqlalchemy numpy
   ```

2. **Instalar Java JDK** (requerido para PySpark)

3. **Iniciar Jupyter:**
   ```bash
   jupyter notebook
   ```

---

## üîç Consultas SQL de Ejemplo

### Precio promedio por activo
```sql
SELECT 
    a.symbol,
    a.name,
    AVG(f.close) as precio_promedio,
    COUNT(*) as total_registros
FROM fact_market_data f
JOIN dim_asset a ON f.asset_id = a.asset_id
GROUP BY a.symbol, a.name
ORDER BY precio_promedio DESC;
```

### Comparar rendimiento entre tipos de mercados
```sql
SELECT 
    a.market_type,
    AVG(f.daily_change_pct) as cambio_promedio,
    AVG(f.volume) as volumen_promedio,
    COUNT(*) as registros
FROM fact_market_data f
JOIN dim_asset a ON f.asset_id = a.asset_id
GROUP BY a.market_type;
```

### Precio m√°ximo y m√≠nimo hist√≥rico por activo
```sql
SELECT 
    a.symbol,
    MIN(f.low) as minimo_historico,
    MAX(f.high) as maximo_historico,
    AVG(f.close) as precio_medio
FROM fact_market_data f
JOIN dim_asset a ON f.asset_id = a.asset_id
GROUP BY a.symbol
ORDER BY maximo_historico DESC;
```

### Top 5 d√≠as con mayor cambio porcentual
```sql
SELECT 
    d.date,
    a.symbol,
    f.daily_change_pct,
    f.open,
    f.close
FROM fact_market_data f
JOIN dim_date d ON f.date_id = d.date_id
JOIN dim_asset a ON f.asset_id = a.asset_id
ORDER BY ABS(f.daily_change_pct) DESC
LIMIT 5;
```

### An√°lisis mensual por tipo de mercado
```sql
SELECT 
    d.year,
    d.month,
    a.market_type,
    AVG(f.close) as precio_promedio,
    AVG(f.volume) as volumen_promedio
FROM fact_market_data f
JOIN dim_date d ON f.date_id = d.date_id
JOIN dim_asset a ON f.asset_id = a.asset_id
GROUP BY d.year, d.month, a.market_type
ORDER BY d.year, d.month;
```

---

## üîÑ Comparaci√≥n Pandas vs PySpark

| Aspecto | Pandas | PySpark |
|---------|--------|---------|
| **Procesamiento** | En memoria | Distribuido |
| **Evaluaci√≥n** | Inmediata | Lazy (diferida) |
| **Mutabilidad** | Mutable | Inmutable |
| **Escalabilidad** | Limitada por RAM | Alta (clusters) |
| **Sintaxis fechas** | `pd.to_datetime()` | `make_date()` + `split()` |
| **Guardado CSV** | `to_csv()` directo | Requiere `toPandas()` en Windows |

---

## üéì Conclusiones

1. **Calidad de datos**: La limpieza de datos es crucial. Se detectaron m√∫ltiples problemas que requer√≠an soluciones espec√≠ficas.

2. **Pandas vs PySpark**: 
   - Pandas es m√°s intuitivo para datasets peque√±os
   - PySpark es necesario para escalabilidad pero requiere m√°s configuraci√≥n en Windows

3. **Modelo dimensional**: El Star Schema facilita consultas anal√≠ticas eficientes con JOINs simples.

4. **Docker**: La contenedorizaci√≥n garantiza reproducibilidad del entorno.

---

## üìö Referencias

- [Documentaci√≥n de Pandas](https://pandas.pydata.org/docs/)
- [Documentaci√≥n de PySpark](https://spark.apache.org/docs/latest/api/python/)
- [SQLite Documentation](https://www.sqlite.org/docs.html)
- [Docker Documentation](https://docs.docker.com/)

---

## üë§ Autor

**Pol Ballarin Costa**  
Proyecto RA1 - Big Data  
Diciembre 2025